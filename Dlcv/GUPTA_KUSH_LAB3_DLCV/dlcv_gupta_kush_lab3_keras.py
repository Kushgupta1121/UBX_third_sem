# -*- coding: utf-8 -*-
"""DLCV_GUPTA_KUSH_lab3_keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uL_F7OxMfPHijhA3UaUGNo9rRdLNyl4B
"""

from __future__ import print_function

#import tensorflow as tf
#import tensorflow.keras
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras import Input
import numpy as np
import matplotlib.pyplot as plt

#print('tensorflow:', tf.__version__)
#print('keras:', tensorflow.keras.__version__)

#load (first download if necessary) the MNIST dataset
# (the dataset is stored in your home direcoty in ~/.keras/datasets/mnist.npz
#  and will take  ~11MB)
# data is already split in train and test datasets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# x_train : 60000 images of size 28x28, i.e., x_train.shape = (60000, 28, 28)
# y_train : 60000 labels (from 0 to 9)
# x_test  : 10000 images of size 28x28, i.e., x_test.shape = (10000, 28, 28)
# x_test  : 10000 labels
# all datasets are of type uint8

#To input our values in our network Dense layer, we need to flatten the datasets, i.e.,
# pass from (60000, 28, 28) to (60000, 784)
#flatten images
num_pixels = x_train.shape[1] * x_train.shape[2]
x_train = x_train.reshape(x_train.shape[0], num_pixels)
x_test = x_test.reshape(x_test.shape[0], num_pixels)

#Convert to float
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

#Normalize inputs from [0; 255] to [0; 1]
x_train = x_train / 255
x_test = x_test / 255

#We want to have a binary classification: digit 0 is classified 1 and 
#all the other digits are classified 0

#print(np.where(y_train==0.0)[0])

y_new = np.zeros(y_train.shape)
y_new[np.where(y_train==0.0)[0]] = 1
y_train = y_new

y_new = np.zeros(y_test.shape)
y_new[np.where(y_test==0.0)[0]] = 1
y_test = y_new

num_classes = 1


#Let start our work: creating a neural network
#First, we just use a single neuron. 

##### TO COMPLETE

"""1. Single Neuron:"""

#Single Neuron 

model = Sequential()
model.add(Input(shape=(num_pixels,)))
model.add(Dense(1, activation="sigmoid"))

model.summary()

#Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# fit the keras model on the dataset
def train_model(X,y,epochs,batch_size):
  hist = model.fit(X, y, epochs=epochs, batch_size=batch_size,verbose=1)
  return hist

X=x_train
y=y_train
epoch=40
batch_size=1000
hist=train_model(X,y,epoch,batch_size)

epochs = range(1, len(hist.history['loss']) + 1)
print(f' Number of Epochs:{epoch}, Batch_size: {batch_size}')
plt.figure(figsize=(10,6))
plt.plot(epochs, hist.history['loss'], '-o', label='Training Loss')
plt.legend()
plt.title('Loss curve: Single Neuron')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.xticks(epochs)
plt.show()

#print(hist.history['loss'])
#print(hist.history['accuracy'])

# evaluate the keras model
X=x_test
y=y_test
_, accuracy = model.evaluate(X, y)
print('Accuracy: %.2f' % (accuracy*100))

"""2. Neural Network with One Hidden Layer:"""

n_neurons=128
model_2 = Sequential()
model_2.add(Input(shape=(num_pixels,)))
model_2.add(Dense(n_neurons, activation='relu'))
model_2.add(Dense(1, activation="sigmoid"))

model_2.summary()

#Complie the model
model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# fit the keras model on the dataset
def train_model(X,y,epochs,batch_size):
  hist_2 = model_2.fit(X, y, epochs=epochs, batch_size=batch_size,verbose=1)
  return hist_2

X=x_train
y=y_train
epoch=40
batch_size=2500
hist_2=train_model(X,y,epoch,batch_size)

epochs = range(1, len(hist_2.history['loss']) + 1)
print(f' Number of Epochs:{epoch}, Batch_size: {batch_size}')
plt.figure(figsize=(10,6))
plt.plot(epochs, hist_2.history['loss'], '-o', label='Training Loss')
plt.legend()
plt.title('Loss curve: NN with one hidden layer')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.xticks(epochs)
plt.show()

# evaluate the keras model
X=x_test
y=y_test
_, accuracy = model_2.evaluate(X, y)
print('Accuracy: %.2f' % (accuracy*100))