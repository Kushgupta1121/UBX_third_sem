# -*- coding: utf-8 -*-
"""DLCV_GUPTA_KUSH_lab3_3_keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nc7VZuqQmLFzFz8dGmoI2_G7D1h0d44o
"""

from __future__ import print_function

#import tensorflow as tf
import tensorflow.keras
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras import Input
import matplotlib.pyplot as plt

#print('tensorflow:', tf.__version__)
#print('tensorflow.keras:', tensorflow.keras.__version__)

#load (first download if necessary) the MNIST dataset
# (the dataset is stored in your home direcoty in ~/.keras/datasets/mnist.npz
#  and will take  ~11MB)
# data is already split in train and test datasets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# x_train : 60000 images of size 28x28, i.e., x_train.shape = (60000, 28, 28)
# y_train : 60000 labels (from 0 to 9)
# x_test  : 10000 images of size 28x28, i.e., x_test.shape = (10000, 28, 28)
# x_test  : 10000 labels
# all datasets are of type uint8

#To input our values in our network Dense layer, we need to flatten the datasets, i.e.,
# pass from (60000, 28, 28) to (60000, 784)
#flatten images
num_pixels = x_train.shape[1] * x_train.shape[2]
x_train = x_train.reshape(x_train.shape[0], num_pixels)
x_test = x_test.reshape(x_test.shape[0], num_pixels)

#Convert to float
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

#Normalize inputs from [0; 255] to [0; 1]
x_train = x_train / 255
x_test = x_test / 255

#Convert class vectors to binary class matrices ("one hot encoding")
## Doc : https://keras.io/utils/#to_categorical
y_train = tensorflow.keras.utils.to_categorical(y_train)
y_test = tensorflow.keras.utils.to_categorical(y_test)

num_classes = y_train.shape[1]

#Let start our work: creating a neural network

##### TO COMPLETE

n_neurons=128
model_3 = Sequential()
model_3.add(Input(shape=(num_pixels,)))
model_3.add(Dense(n_neurons, activation='relu'))
model_3.add(Dense(num_classes, activation="softmax"))

model_3.summary()

#Complie the model
model_3.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy']) #SGD,adam

# fit the keras model on the dataset
def train_model(X,y,epochs,batch_size):
  hist_3 = model_3.fit(X, y, epochs=epochs, batch_size=batch_size,verbose=1)
  return hist_3

X=x_train
y=y_train
epoch=40
batch_size=5000
hist_3=train_model(X,y,epoch,batch_size)

epochs = range(1, len(hist_3.history['loss']) + 1)
print(f' Number of Epochs:{epoch}, Batch_size: {batch_size}')
plt.figure(figsize=(10,6))
plt.plot(epochs, hist_3.history['loss'], '-o', label='Training Loss')
plt.legend()
plt.title('Loss curve: Multi-class NN')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.xticks(epochs)
plt.show()

# evaluate the keras model
X=x_test
y=y_test
_, accuracy = model_3.evaluate(X, y)
print('Accuracy: %.2f' % (accuracy*100))