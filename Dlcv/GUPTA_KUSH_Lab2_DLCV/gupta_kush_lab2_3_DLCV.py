# -*- coding: utf-8 -*-
"""GUPTA_kush_lab2_3_DLCV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z3q2SBesrwjsB4G-npLnrc2YG-gjRWO4
"""

from __future__ import print_function
import numpy as np

#In this first part, we just prepare our data (mnist) 
#for training and testing

#import keras
from tensorflow.keras.datasets import mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

num_pixels = X_train.shape[1] * X_train.shape[2]
X_train = X_train.reshape(X_train.shape[0], num_pixels).T
X_test = X_test.reshape(X_test.shape[0], num_pixels).T
y_train = y_train.reshape(y_train.shape[0], 1)
y_test = y_test.reshape(y_test.shape[0], 1)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
y_train = y_train.astype('float32')
y_test = y_test.astype('float32')
X_train  = X_train / 255
X_test  = X_test / 255

# one-hot encode labels
digits = 10

def one_hot_encode(y, digits):
    examples = y.shape[0]
    y = y.reshape(1, examples)
    Y_new = np.eye(digits)[y.astype('int32')]  #shape (1, 70000, 10)
    Y_new = Y_new.T.reshape(digits, examples)
    return Y_new

y_train=one_hot_encode(y_train, digits)
y_test=one_hot_encode(y_test, digits)

m = X_train.shape[1]

#print(y_train.shape)

#Now, we shuffle the training set
np.random.seed(138)
shuffle_index = np.random.permutation(m)
X_train, y_train = X_train[:,shuffle_index], y_train[:,shuffle_index]

# #Display one image and corresponding label 
import matplotlib
import matplotlib.pyplot as plt
i = 3
print('y[{}]={}'.format(i, y_train[:,i]))
plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)
plt.axis("off")
plt.show()


#Let start our work: creating a neural network

#####TO COMPLETE

"""3. Multiclass Neural Network"""

# loss function - cross entropy (Refer formula above)
def compute_loss(y,y_pred):
  m=y.shape[1]
  loss = -1*(1./m)*(np.sum(np.multiply(np.log(y_pred),y)) + np.sum(np.multiply(np.log(1-y_pred),1-y)))
  return loss

#1 Def of Sigmoid Fun
def sigmoid (x):
  s = 1/(1+np.exp(-x))
  return s

def softmax(x):
  return np.exp(x) / np.sum(np.exp(x), axis=0)

#1. Initalize Weights and Bias
def init_weight(batch_size,nneuron,out_layer):
  w1 = np.random.rand(num_pixels,nneuron) - 0.5
  w2 = np.random.rand(nneuron,out_layer) - 0.5
  #print(w1.shape)
  #print(w2.shape)
  #Bias
  b1 = np.random.rand(nneuron,1) * 0.01
  b2 = np.random.rand(out_layer,1) * 0.01
  return w1,w2,b1,b2

def model(X,w1,w2,b1,b2):

  z1 = np.matmul(w1.T,X)+b1 #input layer. 
  #z1 = w1.T.dot(X)+b
  o1 = sigmoid(z1) #input to hidden layer #(64,1000)
 
  z2=np.matmul(w2.T,o1)+b2 #Hidden layer
  #print('z2 shape',z2.shape)
  y_pred=softmax(z2) #output layer
  
  return y_pred,o1

def back_propagation(X,y,learning_rate,w1,w2,b1,b2):
  
  y_pred,o1=model(X,w1,w2,b1,b2)


  # Compute the loss 
  loss = compute_loss(y,y_pred)  # cross-entropy function
  
  e2 = (y_pred-y)
  e1= np.multiply(np.matmul(w2,e2), np.multiply(o1,1-o1)) #d1

  dw2 = (1./m) * np.matmul(e2,o1.T)
  dw1 = (1./m) * np.matmul(e1,X.T)


  #print('dw1 shape',dw1.shape)
  #print('dw2 shape',dw2.shape)

  db1 = (1./m) * e1

  db2 = (1./m) * np.sum(y_pred-y,axis=1,keepdims=True)


  w1 = w1 - learning_rate*dw1.T
  w2 = w2 - learning_rate*dw2.T
  
  b2 = b2 - learning_rate*db2
  b1 = b1 - learning_rate*db1

  return w1,w2,b1,b2,loss

# Training Model
batch_size=1000
learning_rate=0.5
def train_model(X,y,w1,w2,b1,b2):
  losses=[]
  print(f'Training loss for model:')
  for i in range(0,m,batch_size):
    X_data=X[:,i:i+batch_size]
    y_data=y[:,i:i+batch_size]
    w1,w2,b1,b2,loss=back_propagation(X_data,y_data,learning_rate,w1,w2,b1,b2)
    if(i==0):
      print(f'Epoch {i+1}:', round(loss,4))
    else:
      print(f'Epoch {i/batch_size+1}:',round(loss,4))
    losses.append(loss)
  return losses,w1,w2,b1,b2

# Testing model
#batch_size=1000
n=y_test.shape[1]
y_prediction = np.zeros(y_test.shape)
def test_model(X,y,w1,w2,b1,b2):
  test_losses=[]
  print(f'Test loss for model')
  for i in range(0,n,batch_size):
    X_data=X[:,i:i+batch_size]
    y_data=y[:,i:i+batch_size]
    y_pred_test,o1=model(X_data,w1,w2,b1,b2)
    y_prediction[:,i:i+batch_size]= y_prediction[:,i:i+batch_size]+ y_pred_test
    # Compute the loss
    loss = compute_loss(y_data,y_pred_test)  # cross-entropy function
    if(i==0):
      print(f'Epoch {i+1}:', round(loss,4))
    else:
      print(f'Epoch {i/batch_size+1}:',round(loss,4))
    test_losses.append(loss)
  return test_losses,y_prediction

def calculate_accuracy(y,y_pred):
  accuracy=np.sum(np.argmax(y_pred_test,axis=0) == np.argmax(y_test,axis=0))/y_test.shape[1]
  return accuracy

# Plotting loss
def plot_loss(loss):
  epochs = range(1, len(loss) + 1)
  plt.figure(figsize=(10,6))
  plt.plot(epochs, loss, '-o', label = 'Loss')
  plt.legend()
  plt.title('Loss curve')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.show()

#Initalize weights
w1,w2,b1,b2=init_weight(batch_size,nneuron=64,out_layer=10)

# Train the model, calculate the loss
training_loss,w1,w2,b1,b2 = train_model(X_train,y_train,w1,w2,b1,b2)

print('Train Loss for network')
plot_loss(training_loss)

test_loss,y_pred_test=test_model(X_test,y_test,w1,w2,b1,b2)

print('Test Loss for the network')
plot_loss(test_loss)

cal=calculate_accuracy(y_test,y_pred_test)
print(f'Test accuracy: {cal}')