# -*- coding: utf-8 -*-
"""GUPTA_KUSH_DLCV_lab4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kxnFJj4c0p_QKsw7twR9tz2c7YaR1TxF
"""

import torch
from torch import nn,optim,softmax
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision import transforms
import numpy as np
import matplotlib.pyplot as plt
from torch.nn.modules import flatten

# Get cpu or gpu device for training.
device = "cuda:0" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

training_data = datasets.MNIST(root="./data", download=True,
                                      train=True, 
                                      transform=transforms.ToTensor())

test_data = datasets.MNIST(root="./data", download=True,
                                  train=False,
                                  transform=transforms.ToTensor())
# data is in [0; 1] (thanks to ToTensor()), but there is no "standardisation"

batch_size = 32
train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=2)
test_dataloader = DataLoader(test_data, batch_size=batch_size,shuffle=False, num_workers=2)

#Defining the convolutional layers and the neural network
class Neural_net(nn.Module):
  def __init__(self):
    super(Neural_net,self).__init__()

    # creating the 2D convolution layers
    self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
    self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)

    #creating drop-out layers
    self.dc2= nn.Dropout(0.5)

    #Creating Max-Pooling 
    self.mp1= nn.MaxPool2d(2,stride=2)

    #Fully Connected layer mapping the output of the convolutional layers to 128
    self.fc1= nn.Linear(14*14*16,128)

    #Fully connected layers for 10 classes
    self.fc2 = nn.Linear(128, 10)

  def forward(self,X):
    relu = torch.nn.ReLU()

    #applying convlution and ReLU
    X = relu(self.conv1(X))
    X = relu(self.conv2(X))
    
    #applying Max pooling
    X=self.mp1(X)
    
    #Flattening the inputs
    X = X.reshape(-1,14*14*16)    
    X = relu(self.fc1(X))

    #applying drop out
    X = self.dc2(X)

    output = self.fc2(X)
    
    return output

model= Neural_net()
print(model)

loss_func = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.9)

def train_for_epoch():

    # putting model in train mode
    model.train()

    # keep track of the training losses during the epoch
    train_losses = []

    for batch, targets in train_dataloader:
        
        # clear previous gradient computation
        optimizer.zero_grad()

        # forward propagation
        predictions = model(batch)

        # calculate the loss
        loss = loss_func(predictions, targets)

        # backpropagate to compute gradients
        loss.backward()

        # update model weights
        optimizer.step()

        # update average loss
        train_losses.append(loss.item())

    # calculate average training loss
    train_loss = np.mean(train_losses)

    return train_loss

def train(first_epoch, num_epochs):
    
    train_losses = []

    for epoch in range(first_epoch, first_epoch + num_epochs):

        # training phase
        train_loss = train_for_epoch()      

        print(f'epoch: [{epoch:02d}] train loss: {train_loss:04f}')
        
        train_losses.append(train_loss)
    
    return train_losses

train_loss=train(1,20)

epochs = range(1, len(train_loss) + 1)

plt.figure(figsize=(10,6))
plt.plot(epochs, train_loss, '-o', label='Training loss')
plt.legend()
plt.title('Learning curves')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.xticks(epochs)
plt.show()

def check_accuracy(dataloader):
  # putting model in evaluation mode
    model.eval()
    
    y_pred=[]

    with torch.no_grad():

      #go over each batch to determine model accuracy.

      for batch,_ in dataloader:
        
        # Predict probabilty of each class
        predictions = model(batch)

        #applying softmax on predictions
        predictions = softmax(predictions,dim=1)

        # convert to numpy
        predictions = predictions.numpy()

        # save
        y_pred.append(predictions)
    
    # stack predictions into a (num_samples, 10) array
    y_pred = np.vstack(y_pred)
    return y_pred

# compute predictions on the test set
y_pred = check_accuracy(test_dataloader)

# find the argmax of each of the predictions
y_pred = y_pred.argmax(axis=1)

# get the true labels and convert to numpy
y_true = np.array(test_data.targets)

 # Calculate accuracy as the average number of times y_true == y_pred
accuracy = np.mean(np.equal(y_pred,y_true))

print(accuracy)

error_indicator = y_pred != y_true
error_examples = test_data.data[error_indicator, :, :]
sample = error_examples[:10]

# 10 Worst classifications
fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(20,20))
plt.grid(False)
plt.xticks([])
plt.yticks([])
for i in range(sample.shape[0]):
  axes[i].set_title('True label:'+str(y_true[error_indicator][i])+'\n''Predict_label:'+str(y_pred[error_indicator][i])+'\n')
  axes[i].imshow(sample[i,:,:],cmap='gray')
  
plt.show()